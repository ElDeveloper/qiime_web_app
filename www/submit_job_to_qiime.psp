<%
__author__ = "Doug Wendel"
__copyright__ = "Copyright 2009-2010, Qiime Web Analysis"
__credits__ = ["Doug Wendel"]
__license__ = "GPL"
__version__ = "1.0.0.dev"
__maintainer__ = ["Doug Wendel"]
__email__ = "wendel@colorado.edu"
__status__ = "Development"
%>

<%
from data_access_connections import data_access_factory
from enums import DataAccessType
data_access = data_access_factory(DataAccessType.qiime_production)

def submitJob(study_id, user_id, output_dir):
    # Submit jobs to the queue
    sff_files = data_access.getSFFFiles(study_id)
    mapping_files = data_access.getMappingFiles(study_id)
    split_library_mapping_file = mapping_files[0]

    # Build the input string
    input_string = ''
    sff_file_number = 0
    params = []

    params.append('Mapping=%s' % split_library_mapping_file)
    params.append('Output=%s' % output_dir)
    params.append('SFF=%s' % ','.join(sff_files))
    job_input = '!!'.join(params)
    
    #req.write(job_input)

    # Submit the job
    job_id = data_access.createTorqueJob('ProcessSFFHandler', job_input, user_id, study_id)
    
    #req.write(str(job_id))
    
    # Make sure a legit job_id was created. If not, inform the user there was a problem
    if job_id < 0:
        req.write('<p/>There was an error creating the job. Please contact the system administratorx.')
    else:
        # Redirect to the home page for this study
        psp.redirect('fusebox.psp?page=select_study_task.psp')


# Gather necessary values to create a new queue job
study_id = int(sess['study_id'])
user_id = int(sess['web_app_user_id'])
sff_files = data_access.getSFFFiles(study_id)
mapping_files = data_access.getMappingFiles(study_id)

# Generate a mapping file based on 5 required columns: 'SAMPLE_NAME', 'BARCODE', 'LINKER', 'PRIMER', 'DESCRIPTION'
data_access = data_access_factory(DataAccessType.qiime_production)
study_id = str(sess['study_id'])

# We only want the most minimal mapping file for running split_libraries.py
statement = """
select  sa.sample_name as "#SampleID", 
        sp.barcode as Barcode, 
        concat(sp.linker, sp.primer) as LinkerPrimerSequence, 
        sp.experiment_title as Description
from    study s
        inner join sample sa
        on s.study_id = sa.study_id
        inner join sequence_prep sp
        on sa.sample_id = sp.sample_id
where   s.study_id = %s""" % str(study_id)

# Run the statement
con = data_access.getMetadataDatabaseConnection()
cur = con.cursor()
results = cur.execute(statement)

# Create new mapping file in filesystem
mapping_file_dir = sess['mapping_file_dir']
if not os.path.exists(mapping_file_dir):
    os.makedirs(mapping_file_dir)
    
mapping_file = file(os.path.join(mapping_file_dir, 'split_libraries_mapping_file.txt'), 'w')

# Write the header row
to_write = ''
for column in cur.description:
    to_write += column[0] + '\t'
mapping_file.write(to_write[0:len(to_write)-1] + '\n')

for row in results:
    # Can't use something like '\t'.join(row) because not all items in list
    # are string values, hence the explicit loop structure here.
    to_write = ''
    for column in row:
        val = str(column)
        if val == 'None':
            val = ''
        to_write += val + '\t'
    # Write the row minus the last tab
    mapping_file.write(to_write[0:len(to_write)-1] + '\n')

mapping_file.close()

# Add to the database list
data_access.addMappingFile(study_id, mapping_file.name)

# Set the output_dir
output_dir = sess['study_dir']

# Submit the job to the queue
submitJob(study_id, user_id, output_dir)

%>
